{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2f44db87",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "97d30210",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "05542170",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_size, heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.heads = heads\n",
    "        self.head_dim = embed_size // heads\n",
    "\n",
    "        assert self.head_dim * heads == embed_size, \"Embed size needs to be divisible by heads\"\n",
    "\n",
    "        self.values = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.keys = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.queries = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.fc_out = nn.Linear(heads * self.head_dim, embed_size)\n",
    "\n",
    "    def forward(self, values, keys, query, mask):\n",
    "        N = query.shape[0]\n",
    "        value_len, key_len, query_len = values.shape[1], keys.shape[1], query.shape[1]\n",
    "\n",
    "        # Split embedding into self.heads pieces\n",
    "        values = values.reshape(N, value_len, self.heads, self.head_dim)\n",
    "        keys = keys.reshape(N, key_len, self.heads, self.head_dim)\n",
    "        queries = query.reshape(N, query_len, self.heads, self.head_dim)\n",
    "\n",
    "        values = self.values(values)\n",
    "        keys = self.keys(keys)\n",
    "        queries = self.queries(queries)\n",
    "\n",
    "        # Einsum does matrix multiplications for query*keys for all batches and heads at once\n",
    "        energy = torch.einsum(\"nqhd,nkhd->nhqk\", [queries, keys])\n",
    "\n",
    "        if mask is not None:\n",
    "            energy = energy.masked_fill(mask == 0, float(\"-1e20\"))\n",
    "\n",
    "        attention = torch.softmax(energy / (self.embed_size ** (1 / 2)), dim=3)\n",
    "\n",
    "        out = torch.einsum(\"nhql,nlhd->nqhd\", [attention, values]).reshape(N, query_len, self.heads * self.head_dim)\n",
    "        out = self.fc_out(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4917c4af",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_size, heads, dropout, forward_expansion):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.attention = MultiHeadAttention(embed_size, heads)\n",
    "        self.norm1 = nn.LayerNorm(embed_size)\n",
    "        self.norm2 = nn.LayerNorm(embed_size)\n",
    "\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(embed_size, forward_expansion * embed_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(forward_expansion * embed_size, embed_size),\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, value, key, query, mask):\n",
    "        attention = self.attention(value, key, query, mask)\n",
    "\n",
    "        x = self.dropout(self.norm1(attention + query))\n",
    "        forward = self.feed_forward(x)\n",
    "        out = self.dropout(self.norm2(forward + x))\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f3dab8f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, embed_size, num_layers, heads, forward_expansion, dropout, device):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.layers = nn.ModuleList([\n",
    "            TransformerBlock(\n",
    "                embed_size,\n",
    "                heads,\n",
    "                dropout=dropout,\n",
    "                forward_expansion=forward_expansion,\n",
    "            )\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        out = x\n",
    "        for layer in self.layers:\n",
    "            out = layer(out, out, out, mask)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "80133c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "transformer = Transformer(embed_size=512, num_layers=6, heads=8, forward_expansion=4, dropout=0.1, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8880b96b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output Tensor:\n",
      "tensor([[[-2.4236e+00,  2.3581e-01, -3.6038e-01,  ..., -9.7450e-02,\n",
      "          -1.1645e-01,  3.3492e-01],\n",
      "         [-2.1384e-02,  1.0151e+00, -3.0742e+00,  ...,  1.0237e-01,\n",
      "          -1.3232e+00,  4.4146e-01],\n",
      "         [ 0.0000e+00, -7.6741e-02, -4.2155e-02,  ..., -1.3690e+00,\n",
      "          -2.1432e+00,  8.8490e-01],\n",
      "         ...,\n",
      "         [-0.0000e+00, -1.3314e+00,  3.2071e-01,  ..., -8.1643e-01,\n",
      "           1.9602e+00, -7.5400e-03],\n",
      "         [-1.6196e-01, -3.0135e-01, -2.4847e-01,  ..., -2.5513e+00,\n",
      "          -9.2246e-01,  0.0000e+00],\n",
      "         [-9.4961e-02,  8.6847e-02,  3.7584e-01,  ..., -0.0000e+00,\n",
      "          -3.1179e-01,  0.0000e+00]],\n",
      "\n",
      "        [[ 2.1049e+00, -4.5234e-01, -0.0000e+00,  ..., -8.3429e-01,\n",
      "          -9.1045e-01,  1.7835e-01],\n",
      "         [-2.8866e-01, -4.8122e-02,  4.4013e-01,  ..., -5.4069e-01,\n",
      "          -1.2793e+00, -0.0000e+00],\n",
      "         [-5.6460e-01, -4.9123e-03,  5.9178e-01,  ..., -0.0000e+00,\n",
      "           2.7341e-01,  4.8724e-01],\n",
      "         ...,\n",
      "         [-1.5256e+00,  6.1962e-02,  3.1952e-02,  ...,  2.6526e-03,\n",
      "          -1.1898e-01,  0.0000e+00],\n",
      "         [ 3.7999e-01, -8.3841e-02,  0.0000e+00,  ...,  5.8416e-01,\n",
      "          -1.2283e+00, -6.2258e-01],\n",
      "         [ 2.2061e-02, -3.3757e-02,  0.0000e+00,  ..., -6.5921e-01,\n",
      "          -4.2996e-01,  3.9520e-01]],\n",
      "\n",
      "        [[ 7.4881e-02, -9.0777e-01, -2.5021e-01,  ..., -2.1847e-01,\n",
      "           4.1372e-01, -3.4541e-03],\n",
      "         [-4.9614e-01, -1.0647e+00,  1.0405e+00,  ..., -9.8668e-01,\n",
      "           1.8559e+00, -4.6095e-02],\n",
      "         [-0.0000e+00, -5.7298e-01,  7.1252e-02,  ..., -3.6946e-01,\n",
      "          -5.6282e-01, -9.4838e-02],\n",
      "         ...,\n",
      "         [-1.3902e+00,  2.3567e-01,  3.0332e+00,  ..., -1.0683e+00,\n",
      "          -1.8543e-01, -4.7248e-01],\n",
      "         [-2.5469e-01, -0.0000e+00, -6.7714e-02,  ..., -7.7501e-01,\n",
      "          -1.7630e-01,  9.2373e-01],\n",
      "         [-6.3151e-01, -3.2647e-01, -9.8338e-01,  ..., -2.8300e-01,\n",
      "          -4.6032e-01, -2.2554e-01]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-9.9752e-01, -1.1828e-01,  2.9043e-01,  ..., -6.7620e-02,\n",
      "           1.4997e-02,  9.2117e-01],\n",
      "         [-4.8144e-01, -8.4350e-01,  5.1629e-01,  ..., -1.1988e+00,\n",
      "          -5.3826e-02, -6.3118e-02],\n",
      "         [ 0.0000e+00, -2.0183e-01, -3.6626e-01,  ...,  3.0844e+00,\n",
      "           2.4064e+00,  0.0000e+00],\n",
      "         ...,\n",
      "         [-4.6609e-01, -2.7628e-01, -2.6059e-01,  ...,  3.7180e-01,\n",
      "          -3.4148e+00,  2.4422e-01],\n",
      "         [ 1.6468e-01,  1.9441e-01, -2.5157e-01,  ...,  6.4569e-01,\n",
      "          -4.4819e-01,  5.5867e-01],\n",
      "         [-6.2697e-04, -6.4617e-01,  2.3377e+00,  ..., -2.6875e-01,\n",
      "          -1.6513e-01,  8.1160e-02]],\n",
      "\n",
      "        [[-0.0000e+00,  1.0281e+00,  6.4462e-01,  ..., -4.5133e-01,\n",
      "          -3.5290e-01,  0.0000e+00],\n",
      "         [ 1.9957e-02, -7.3227e-02,  1.7970e-01,  ..., -1.1424e-01,\n",
      "          -3.3215e-01, -3.1468e+00],\n",
      "         [-1.1121e-01,  2.9604e-01,  1.5027e-01,  ...,  6.9610e-02,\n",
      "          -1.5412e-01,  1.1410e+00],\n",
      "         ...,\n",
      "         [-2.5833e+00, -7.4777e-02,  7.6085e-01,  ..., -1.5559e+00,\n",
      "           0.0000e+00,  8.2323e-02],\n",
      "         [-2.9471e-01, -2.2879e+00,  9.3843e-01,  ..., -3.7431e-01,\n",
      "          -1.2560e+00,  4.2621e-02],\n",
      "         [-8.1630e-02, -1.1100e+00, -3.6641e-01,  ..., -0.0000e+00,\n",
      "          -7.4347e-01,  1.1756e+00]],\n",
      "\n",
      "        [[ 0.0000e+00, -7.1870e-01, -2.3401e-01,  ...,  1.6831e+00,\n",
      "          -2.9402e-01, -2.2893e+00],\n",
      "         [ 1.3910e-01, -7.9322e-01,  1.9550e-01,  ..., -1.8955e-01,\n",
      "           2.7998e-01,  1.7125e-01],\n",
      "         [-1.9517e-01, -3.0469e-01,  2.1798e-02,  ...,  1.0720e+00,\n",
      "          -5.0790e-01, -1.5596e+00],\n",
      "         ...,\n",
      "         [-6.7675e-01,  1.7172e-01, -3.2076e-01,  ..., -4.0325e-01,\n",
      "          -5.2735e-02,  3.9834e-01],\n",
      "         [-5.1495e-01,  3.5897e-02,  9.7641e-01,  ...,  9.7004e-02,\n",
      "          -0.0000e+00, -2.0952e-01],\n",
      "         [ 1.1406e+00, -3.0915e-01, -2.0832e-01,  ..., -6.0136e-01,\n",
      "          -1.6996e+00, -1.1852e+00]]], grad_fn=<MulBackward0>)\n",
      "Output Shape: torch.Size([32, 10, 512])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Example tensor (batch_size, sequence_length, embed_size)\n",
    "x = torch.randn((32, 10, 512))\n",
    "mask = None  # Define mask if needed\n",
    "\n",
    "transformer.to(device)\n",
    "x = x.to(device)\n",
    "\n",
    "# Forward pass through the transformer\n",
    "out = transformer(x, mask)\n",
    "\n",
    "# Printing the output tensor\n",
    "print(\"Output Tensor:\")\n",
    "print(out)\n",
    "print(\"Output Shape:\", out.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f3c6673",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
